<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://mohuazhan.github.io/</id>
    <title>小莫的博客</title>
    <updated>2019-06-28T05:58:28.792Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://mohuazhan.github.io/"/>
    <link rel="self" href="https://mohuazhan.github.io//atom.xml"/>
    <subtitle>以学会友</subtitle>
    <logo>https://mohuazhan.github.io//images/avatar.png</logo>
    <icon>https://mohuazhan.github.io//favicon.ico</icon>
    <rights>All rights reserved 2019, 小莫的博客</rights>
    <entry>
        <title type="html"><![CDATA[一个程序爱好者的简历]]></title>
        <id>https://mohuazhan.github.io//post/CV</id>
        <link href="https://mohuazhan.github.io//post/CV">
        </link>
        <updated>2019-06-28T05:47:37.000Z</updated>
        <content type="html"><![CDATA[<h3 id="个人基本信息">个人基本信息</h3>
<table>
<thead>
<tr>
<th>姓名</th>
<th>性别</th>
<th>年龄</th>
<th>身高</th>
<th>籍贯</th>
</tr>
</thead>
<tbody>
<tr>
<td>莫华瞻</td>
<td>男</td>
<td>25</td>
<td>167cm</td>
<td>广东省廉江市</td>
</tr>
</tbody>
</table>
<h3 id="学历信息">学历信息</h3>
<table>
<thead>
<tr>
<th>学历</th>
<th>毕业院校</th>
<th>专业</th>
<th>院校所在地</th>
</tr>
</thead>
<tbody>
<tr>
<td>大学本科</td>
<td>华北科技学院</td>
<td>应用统计学</td>
<td>河北廊坊</td>
</tr>
</tbody>
</table>
<h3 id="技能信息">技能信息</h3>
<table>
<thead>
<tr>
<th>开发语言</th>
<th>常用框架</th>
<th>其他工具</th>
<th>方向</th>
<th>入门时长</th>
</tr>
</thead>
<tbody>
<tr>
<td>python2.7&amp;3.5</td>
<td>Scrapy，Flask，Django</td>
<td>SPSS，Access，R，fiddler</td>
<td>爬虫，数据分析，人工智能</td>
<td>8个月</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>开发环境</th>
<th>数据库</th>
<th>中间件</th>
<th>分布式监控</th>
<th>协作</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux</td>
<td>MySQL，MongoDB，Redis</td>
<td>RabbitMQ</td>
<td>Zabbix</td>
<td>Git</td>
</tr>
</tbody>
</table>
<h3 id="其他信息">其他信息</h3>
<table>
<thead>
<tr>
<th>政治面貌</th>
<th>工作经验</th>
<th>技能证书</th>
<th>语言</th>
<th>电子邮箱</th>
</tr>
</thead>
<tbody>
<tr>
<td>中共党员</td>
<td>一年</td>
<td>无</td>
<td>普通话，粤语</td>
<td>919656720@qq.com</td>
</tr>
</tbody>
</table>
<h2 id="大学四年20142018">大学四年（2014—2018）</h2>
<ul>
<li><strong>负责</strong>：大一期间当班长，注重班级学风，经常组织班级活动，也是班上的「装软件小能手」</li>
<li><strong>积极</strong>：在学院晚会上两次出演小品和一次合唱；连续三年参加校运会短跑接力项目</li>
<li><strong>勤勉</strong>：四年内两次获得二等奖学金，两次获得三等奖学金</li>
</ul>
<h2 id="工作一年-in-杭州">工作一年（ In 杭州 ）</h2>
<h3 id="深圳怡亚通供应链有限公司金服平台-杭州省区">深圳怡亚通供应链有限公司金服平台（ 杭州省区 ）</h3>
<pre><code>在职时间：2018年7月12日——2018年9月17日

职务：见习信贷经理

业绩：0 单
</code></pre>
<h4 id="见习经历">见习经历：</h4>
<p>杭州省区是EA金服开辟不久的职场，存量客户不多。分配带我的老师傅在我入职一个星期后便离职了，在没有人帮带的情况下，我积极向另一位经验丰富的信贷经理请教，至今仍然感谢他当时的悉心传授。</p>
<p>每周我坚持跑三天地推，从一天10家到一天30家，杭州江干区到下城区的商超和各大副食品市场都留下我的足迹，每天晚上都会整理优化小本本上的话术，每次「出征」前都会在脑海中多次模拟和对方沟通的场景；平时有遇到合作平台提供的名单，我会积极主动去电销，过后反复听通话记录，也虚心向同事请教话术技巧。</p>
<p>可惜好景不长，9月初怡亚通宣布大规模裁员，我也在「优化名单」内。剩下的半个月时间，我努力去执行营销方案，却无法收获一笔信贷业务，于是在9月17日提请离职。</p>
<h3 id="杭州壹马科技有限公司秒派助手">杭州壹马科技有限公司（<a href="http://dsp.banmaim.com">秒派助手</a>）</h3>
<pre><code>在职时间：2018年9月18日至今

职务：打杂
</code></pre>
<h4 id="工作经历">工作经历：</h4>
<ul>
<li><strong>文本分类器（去年9月—10月）</strong>：</li>
</ul>
<table>
<thead>
<tr>
<th>环境</th>
<th>语言</th>
<th>领域</th>
<th>常用库</th>
<th>算法</th>
<th>参考</th>
</tr>
</thead>
<tbody>
<tr>
<td>win7</td>
<td>python</td>
<td>NLP</td>
<td>jieba，numpy，pandas</td>
<td>tf&amp;idf，textrank</td>
<td><a href="http://www.doc88.com/p-332769038066.html">Tmsvm</a></td>
</tr>
</tbody>
</table>
<p>入职后第一次接触python，当时公司正在给一款智能媒资产品设计文本分析模块，要实现给新闻文本分类的功能。我从NLP自然语言处理这一块入门，下载的数据集是清华新闻分类语料。先是对比云平台和开源（如：jieba，StanfordCoreNLP，pyltp等）的分词能力，然后手写tf&amp;idf算法抽取中心词，文本摘要和中心词提取也有使用第三方库。最后是用的分类算法是参考了浙江大学张知临先生开源的基于SVM的线性分类器源码。</p>
<p>整个训练花了半个月的时间（公司设备条件较落后，由于RAM有限，最终确定训练样本量为70万条新闻），平均每两天产出一个模型。我用训练出来的模型进行预测，根据准确率和召回率调优参数，反复迭代，最终分类器准确率达到82%（预测样本1000条）。</p>
<blockquote>
<p><em>总结：当时尚未熟悉深度学习的前沿资讯，后来又继续了解了TensorFlow框架结合CNN和RNN算法做的文本分类。</em></p>
</blockquote>
<ul>
<li><strong>爬虫 &amp; 数据分析 &amp; 算法设计（去年11月—12月）</strong>：</li>
</ul>
<table>
<thead>
<tr>
<th>环境</th>
<th>语言</th>
<th>领域</th>
<th>常用库</th>
<th>数据库</th>
<th>数据量级</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux</td>
<td>python</td>
<td>爬虫，数据分析</td>
<td>requests，selenium，pandas，matplotlib，wsgiref，urllib2</td>
<td>MongoDB</td>
<td>十万级</td>
</tr>
</tbody>
</table>
<p>11月，公司要开展短视频平台的数据中心开发，招了一个爬虫工程师，我跟着他学习。公司要求爬取抖音，快手，秒拍和微博等平台的短视频，我负责快手和秒拍的数据爬取，以单个账号为单位，爬取该账号下的视频以及点赞评论等互动数据。爬取任务每小时执行一次，互动数据为列表递增。</p>
<p>12月，根据首批爬取的数据进行数据分析，分析所有平台的账号的数据变化趋势，用聚类分析按互动数据给账号分层次，从而调整各层次账号的爬取频率。然后设计了一套算法，计算单条视频当前的热度指数，飙升指数以及单个账号的传播力指数。（先考虑各项指数涉及的数据维度，根据指数展示的需求，进行数值拟合，调优参数，最终计算各维度权重。在算法中，考虑数据对齐和时间因素，参数与权重会动态调整）</p>
<blockquote>
<p><em>总结：爬虫并未涉及到 js解密和 APP逆向破解，代理和加签算法分别向服务商购买的，我在这期间仅入门了最基本的爬虫。算法设计方面，我在今年5月份再次进行了优化</em></p>
</blockquote>
<ul>
<li><strong>撰写公众号文章 &amp; 视频剪辑 &amp; 社群运营 &amp; 微信机器人（去年12月底—今年1月）</strong>：</li>
</ul>
<table>
<thead>
<tr>
<th>平台</th>
<th>任务</th>
<th>工具</th>
</tr>
</thead>
<tbody>
<tr>
<td>微信公众号</td>
<td>写文章，剪视频</td>
<td>python(数据处理)，Excel，Adobe Premiere</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>项目</th>
<th>语言</th>
<th>使用库</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>微信机器人</td>
<td>python</td>
<td>itchat，requests</td>
<td>实时查看各类榜单，分享链接获取抖音无水印视频</td>
</tr>
</tbody>
</table>
<p>1月，公司开始运营微信公众号（<strong>黑马数据助手</strong>），我负责给公众号写文章（文章中包含用爬取来的素材剪辑的视频）。</p>
<p>如：<a href="https://mp.weixin.qq.com/s?__biz=MzUzOTk5Njg5MQ==&amp;mid=2247484272&amp;idx=1&amp;sn=cb797aaa8855e33bc429073b62d56e90&amp;chksm=fb3ea4c4cc492dd2b4084395b6b7dd076890664508c9b909e56008761658ec5bc5f07862ddb2&amp;mpshare=1&amp;scene=1&amp;srcid=0618ytoPDBGlxlen4aMqOdX5&amp;from=singlemessage&amp;ascene=1&amp;devicetype=android-26&amp;version=2700033c&amp;nettype=ctnet&amp;abtest_cookie=BQABAAoACwASABMAFQAGACOXHgBWmR4AvpkeANyZHgD0mR4AC5oeAAAA&amp;lang=zh_CN&amp;pass_ticket=2%2BNK42CAdH9Mshaaop6lSYq1KYIbHvWqJzAZuwH2RPEfYOVWqoRsbddQciWgfNj4&amp;wx_header=1">人民日报的「抖音一分钟」</a>和 <a href="https://mp.weixin.qq.com/s?__biz=MzUzOTk5Njg5MQ==&amp;mid=2247484308&amp;idx=1&amp;sn=a7f836a51b6f6f79e72cafa212bcaf4b&amp;chksm=fb3ea420cc492d364ae57cf3a4bbb41f13d8ea1343ec51ee53fbf81adaa16abedcf2a5a2c057&amp;scene=4&amp;subscene=126&amp;ascene=0&amp;devicetype=android-26&amp;version=2700033c&amp;nettype=ctnet&amp;abtest_cookie=BQABAAoACwASABMAFQAGACOXHgBWmR4AvpkeANyZHgD0mR4AC5oeAAAA&amp;lang=zh_CN&amp;pass_ticket=2%2BNK42CAdH9Mshaaop6lSYq1KYIbHvWqJzAZuwH2RPEfYOVWqoRsbddQciWgfNj4&amp;wx_header=1">盘一盘抖音「人气最高」职业</a></p>
<p>同时，公众号文章会将读者引流到微信群（面向媒体人），我在执行<strong>拉新—&gt;促活—&gt;留存</strong>这套流程的同时，编写了一个微信机器人进行群管理（我分了两个模块来写，一部分是API组，用于实现功能；另一部分是请求调用组，用于接受群信息并返回准确信息），用于帮助群成员实时查询使用。</p>
<blockquote>
<p><em>总结：公众号吸引了不少搞融媒体的业内人士关注，我在这个过程中进一步接触了媒体行业，5G的到来和 AI 逐步成熟为融媒体发展深度赋能</em></p>
</blockquote>
<ul>
<li><strong>社群运营 &amp; 数据运营 &amp; SEM优化 &amp; 内容推广（今年3月至今）</strong>：</li>
</ul>
<table>
<thead>
<tr>
<th>社群规模</th>
<th>推广平台</th>
<th>用户类别</th>
<th>月均用户增长</th>
<th>日均活跃用户</th>
</tr>
</thead>
<tbody>
<tr>
<td>300人</td>
<td>微信，抖音，百度等</td>
<td>广告主，达人&amp;MCN机构</td>
<td>500+</td>
<td>100+</td>
</tr>
</tbody>
</table>
<p>公司的SaaS产品2.0——<a href="http://dsp.banmaim.com">秒派助手</a>在4月10号上线。在上线前，我针对产品面向的用户群体，撰写业务逻辑，通过微信，抖音等社交平台寻找机构进行合作推广。</p>
<p>正式上线后，我通过社群运营给用户进行商务对接，吸引关注抖音变现的群体来使用产品。一方面通过挖掘需求来优化产品，提高用户粘性；另一方面，在运营中寻找一套可以形成闭环的以数据产品为基础，商业模式为核心的业务逻辑。</p>
<p>同时，我也负责产品的百度SEM推广和官网的内容编辑，每天结合GA统计分析和后台数据产出数据运营报表。</p>
<blockquote>
<p><em>总结：运营方法<strong>出台—&gt;探索实践—&gt;总结—&gt;优化</strong>的周期短灵活高，我得以及时思考和调整。但是，团队缺少有经验的运营管理者指导，运营效率较低，在瓶颈期也难以突破</em></p>
</blockquote>
<ul>
<li><strong>抖音账号抓取（今年3月）</strong>：</li>
</ul>
<p>|平台|工具|数据库|数据量级|效率|
|-- |-- |-- |-- |
|抖音|fiddler，python|MongoDB|十万级|日均10000+|</p>
<p>优质的不同类别的短视频账号是秒派助手数据平台的主要监控对象。按照平台对账号的甄选条件，我通过fiddler抓包在抖音平台上先后抓取了超过10万个账号（带分类）的数据，提供给公司使用。</p>
<blockquote>
<p><em>总结：如果先是全平台爬取，后进行账号分类，效率应该会更高。当时在有限的设备和网络资源基本上，无法评估全量爬取的用时，因此修改方案</em></p>
</blockquote>
<ul>
<li><strong>商品分类器（非图像识别，3月底v1.0版，5月升级v2.0版）</strong>：</li>
</ul>
<table>
<thead>
<tr>
<th>语音</th>
<th>领域</th>
<th>使用库&amp;框架</th>
<th>数据量级</th>
</tr>
</thead>
<tbody>
<tr>
<td>python</td>
<td>爬虫，NLP，Web</td>
<td>requests，multiprocessing，jieba；flask</td>
<td>百万级</td>
</tr>
</tbody>
</table>
<p>公司爬取回来的商品需要带有分类。3月份，我提取北京大学开放数据中心的电商数据集的所有商品名称及对应分类，训练了一个商品分类模型，并集成API交付给后端使用。</p>
<p>5月初，我抽空重新训练调优该模型。我从京东爬取了230万条商品信息（西刺免费代理，开了4个线程跑），利用商品名称及京东商品分类标准，重新训练好新的分类模型并部署使用。</p>
<blockquote>
<p><em>总结：商品分类器仍可以通过以下方法再优化：新增图像识别；维护商品词库；校准判别阈值等</em></p>
</blockquote>
<ul>
<li><strong>星图平台爬虫系统搭建（今年5月底）</strong>：</li>
</ul>
<p>星图平台为抖音用于商务对接的官方平台，其数据维度极具参考价值。应公司要求，我单独搭建星图平台的爬虫系统。</p>
<p>项目雏形可见文章：<a href="https://mohuazhan.github.io/post/liao-yi-ge-jian-dan-de-pa-chong/">聊一个简单的爬虫</a></p>
<blockquote>
<p><em>总结：首先该爬虫考虑的主要是前期使用，对于后期数据增长的处理，在系统中并没有体现。弊端有：爬虫非分布式，尽管消息队列做了持久化，宕机后仍需人工处理，不能及时切换；增量式爬取数据极快，数据库未搭建集群，也无切片处理；在只有一个 cookie 的情况下，未尝试使用代理，爬虫效率低；cookie 到期时未设置发送邮件提醒，会造成一定程度的数据缺失；在这里用 Flask 编写的 API 使用了 gevent 来处理异步非阻塞，并未进行压测，在高并发时应对比Tornado，Flask，Django（使用 gunicon/uwsgi + supeivisor + nginx 部署），nameko（RPC协议）四者的性能；还有一些关于 MQ 和 Redis 的使用上，都有不到位的地方</em></p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[聊一个简单的爬虫]]></title>
        <id>https://mohuazhan.github.io//post/liao-yi-ge-jian-dan-de-pa-chong</id>
        <link href="https://mohuazhan.github.io//post/liao-yi-ge-jian-dan-de-pa-chong">
        </link>
        <updated>2019-06-07T09:49:24.000Z</updated>
        <content type="html"><![CDATA[<p><em>最近牙疼，吃什么都不香，为了省点拔牙钱，排队办市民卡，郁闷，于是写文章解闷。</em></p>
<p><strong>嗯……那就从一个简单的爬虫开始吧。</strong></p>
<blockquote>
<p>运行环境：Ubuntu 16.04</p>
<p>语言：Python2.7</p>
<p>数据库：Redis，mongoDB</p>
<p>中间件：RabbitMQ</p>
<p>进程管理：Supervisor</p>
</blockquote>
<p>如无必要，勿增实体。为什么要加这么多花里胡哨的东西来占用资源？咳咳，就是为了玩玩。</p>
<h2 id="需求文档">需求文档：</h2>
<ul>
<li>1.爬取某平台达人数据，包含主页数据和视频粉丝等详情数据；</li>
<li>2.每周更新一次数据；</li>
<li>3.可随时插入或更新某达人详情数据。</li>
</ul>
<p><strong>画个巨Low的架构图来看看：</strong>
<img src="https://mohuazhan.github.io//post-images/1561024672343.png" alt=""></p>
<h2 id="技术评估">技术评估：</h2>
<p><strong>1.爬取频率</strong>：测测测！不要盲目自信设个延迟就溜了！需求一实际上是最基础的数据爬取和存储，还没涉及到数据处理层。但由于需要登录的cookie我只有一个，爬取频率要控制好（请求后延迟或消费者回调延迟都可以），线程嘛也不敢多开（python的multiprocessing模块中pool() + pool.map()开启多线程很方便，谨慎食用咯）。而多条MQ队列同时发布和消费本身就是多线程啊，How to DO？切记，如果没有十足的把握（让爬虫足够稳定），别把不同的消费者写在同一个脚本里。除非……把爬虫脚本写成API，通过发送请求来回调不同的队列消息，效果也不错，这样甚至能够通过设置请求频率来控制爬取频率。</p>
<p><strong>2.数据去重</strong>：每周更新一次数据意味着每周都要爬一次该平台的所有达人，一般每周新增的达人不多，所以要在爬取时做去重处理。首先说一下为什么选择了mongoDB而不是MySQL？由于该平台爬取到的是json数据（包含list），故用mongoDB存储再好不过（懒人必备，这样就不用考虑存哪些字段，全拿下就行了，后期再按需求处理）。而对于json数据的处理，尽可能和爬取分开，在爬取过程中批量处理结构化数据，会非常影响爬虫效率，又或者，将爬取和数据处理分开在不同线程处理（切勿让数据处理延长单个线程的时间），可服务器资源有限的前提下这并不是一个高效的方法。既然不用MySQL，那去重我选择Redis。用极限思维来考虑，当爬虫级别上千万以后（这里还没考虑高并发读写），每一次查询都会消耗不少时间，爬虫效率也会逐渐降低，所以要在读写上尽可能提高效率。把键值对存到Redis的哈希表（上锁）来实现去重，效果可以媲美MySQL。</p>
<p><strong>3.稳定性</strong>：有这么多花里胡哨的东西，稳定性要考虑的可多咯。首先是爬虫的稳定性，要控制频率，关于重试和超时，不要设置多次重试，就这一个cookie，失败了赶紧把URL重新塞回队列，等下一次爬取就行（非常有必要延迟一段时间后再启动下一次）。如果更加谨慎，可以新增一条与原队列延迟不同的「回笼」队列，来专门处理异常请求。RabbitMQ的话要考虑宕机的问题（谁知道什么时候挂机），这是要做好队列持久化；不小心断开连接也要做好信息不丢失的处理，没处理完的消息，保持下次连上的时候发布者能够重新发送；还有非常非常重要的——自动连接，如果想省心地挂上爬虫就和朋友去嗨皮，最好让RabbitMQ不检测心跳，这样就不会造成服务端主动断开连接（特别是延迟较大的时候）。以上几点也是我不采用Python自带的Queue模块的原因，显然使用中间件对队列的可控性更高，可灵活应对更多特殊情况。</p>
<p><strong>4.灵活性</strong>：这是针对可随时插入或更新某条数据而言的。比方说，距离上次数据更新已经过去一星期了，公司运营小姐姐想要最新的某个达人的报价，怎么办？就这个问题我可以新增一条队列，叫「应急」队列什么的都行。这时她可以把这个达人的ID扔到「应急」队列中优先爬取（RabbitMQ的界面化操作非常友好），但这会有一个问题，万一不小心输错ID了怎么办？RabbitMQ可是照样发送的呀。不行不行，为了让运营部门省心，我可以开放一个API接口，当ID插入队列之前，我会判断该达人ID是否存在于达人库中，如果不存在该达人则不插入该消息。同理，公司不同岗位的同事的数据需求都可能不同，可按需开放接口。涉及到共用一个数据库，还要考虑如何去避免数据混淆（可构造列表存放不同时间维度的数据，同时也是为了处理时数据对齐）。</p>
<p><strong>5.资源分配</strong>：如果可以的话，RabbitMQ一台，Redis一台，爬虫一台，数据库一台，代理的话，免费的可以用西刺（很不稳定）……当然，我只是玩玩，我全部用一台，嘻嘻。</p>
<h2 id="程序设计">程序设计</h2>
<p><em>来了，要开始了，兄弟</em></p>
<h3 id="创建虚拟环境">创建虚拟环境</h3>
<p>用python做项目要养成各项目间环境独立的习惯（当然如果使用的是docker那就方便多了）
现在我创建一个名为/spiders的文件夹并在该文件下创建名为env_py2.7虚拟环境：</p>
<pre><code class="language-shell">root@ubuntu:~# cd /spiders
root@ubuntu:/spiders# virtualenv env_py2.7
</code></pre>
<p>激活环境与退出环境：</p>
<pre><code class="language-shell">root@ubuntu:/spiders# source env_py2.7/bin/activate
(env_py2.7) root@ubuntu:/spiders# deactivate
</code></pre>
<h3 id="创建项目并安装所有依赖">创建项目并安装所有依赖</h3>
<p>在/spiders文件夹下创建/xingtu文件夹，代码和配置等就放在这下面了。</p>
<pre><code class="language-shell">root@ubuntu:/spiders/xingtu# tree
.
├── agent_proxy.py		# 此文件存放user-agents和代理
├── cookie.txt			# 此文件存放cookie
├── get_kol_detail.py	# 爬取达人详细数据的脚本
├── get_kol_list.py		# 爬取达人列表
├── requirements.txt	# 打包依赖
├── send_id_api.py		# 更新达人数据的接口
├── send_pages.py		# 发布url消息
└── xingtu_supervisord.conf		# 进程管理配置

0 directories, 8 files
</code></pre>
<p>这个小爬虫要用到的库及作用如下，我把所有依赖写进requirements.txt：</p>
<pre><code class="language-txt">pika	# 连接操作rabbitmq
requests	# 爬虫
redis	# 连接redis
pymongo	# 连接mongoDB
flask	# 编写API
flask_cors
pandas	# 数据处理
gevent	# 并发处理
</code></pre>
<p>在虚拟环境中直接安装所有依赖：</p>
<pre><code class="language-shell">(env_py2.7) root@ubuntu:~# pip install -r /spiders/xingtu/requirements.txt
</code></pre>
<h3 id="连接rabbitmq">连接RabbitMQ</h3>
<p>记得把heartbeat设为0，省去手动重连的后顾之忧：</p>
<pre><code class="language-python">credentials = pika.PlainCredentials(账号, 密码)
connection = pika.BlockingConnection(pika.ConnectionParameters(host='服务器IP',
                                                               credentials=credentials,
                                                               heartbeat=0 ))   # 设置heartbeat为0，意味着不检测心跳，server端不会主动断开连接
channel = connection.channel()
</code></pre>
<h3 id="创建发布队列发送urlsend_pagespy">创建发布队列，发送URL(send_pages.py)</h3>
<p>创建一个名为'xingtu_page'的队列来开始发布我们的爬取链接吧：</p>
<pre><code class="language-python">def get_page(page):
    # url = '爬取 {} 的URL'.format(page)		# 更新url的page参数
    channel.queue_declare(queue='xingtu_page', durable=True)
    channel.basic_publish(exchange='',
                          routing_key='xingtu_page',
                          body=url,
                          properties=pika.BasicProperties(
                              delivery_mode=2,  # 消息持久化
                          ))

if __name__ == '__main__':
    pool = Pool(1) #只开一个线程
    pool.map(get_page, range(1,2000))	#多线程工作
    pool.close()
    pool.join()
    connection.close()
</code></pre>
<h3 id="爬取达人列表get_kol_listpy">爬取达人列表(get_kol_list.py)</h3>
<p>关于爬取中的超时和重试，可以自己定义，这里直接用requests封装好的：</p>
<pre><code class="language-python">req = requests.Session()
req.mount('http://', HTTPAdapter(max_retries=1))
req.mount('https://', HTTPAdapter(max_retries=1))
</code></pre>
<p>消费者回调函数:</p>
<pre><code class="language-python">def callback(ch, method, properties, body):
    try:
        '''调用get_kol函数爬取'''
        get_kol_id(body)
        time.sleep(20)
        ch.basic_ack(delivery_tag=method.delivery_tag)  # 消费者每次收到消息，要通知一声：已经收到，如果消费者连接断了，rabbitmq会重新把消息放到队列里，下次消费者可以连接的时候，就能重新收到丢失消息
    except:
        '''请求失败则把url重新放回队列'''
        channel.basic_publish(exchange='',
                              routing_key='xingtu_page',
                              body=body,
                              properties=pika.BasicProperties(
                                  delivery_mode=2,  # 消息持久化
                              ))
        time.sleep(120)	# 爬取失败就等两分钟再继续
</code></pre>
<p>爬取并且发布消息到新队列：：</p>
<pre><code class="language-python">def get_kol_id(url):
    headers = {'User-Agent': agent_proxy.get_agent()}
    cookies = open(u'/spiders/xingtu/cookie.txt','r').read()	# cookie写在cookie.txt里面
    cookies_dict = {}
    '''对原始cookies构造字典格式'''
    for line in cookies.split(';'):
        key, value = line.split('=', 1)  # 1代表只分一次，得到两个数据
        key = key.replace(' ', '')
        cookies_dict[key] = value
    list_kol = req.get(url, headers=headers, cookies=cookies_dict, timeout=20, verify=False).json() # 这里使用req是因为上面对resquests设置了重试机制
    for author in list_kol['data']['authors']:
        id_xingtu = author['id']
        id_douyin = author['core_user_id']
        if my_redis.hget('hash_kolid', id_douyin) == None:  # redis 通过查询hash表去重
            my_redis.hsetnx('hash_kolid', id_douyin, id_xingtu)  # 将id写入redis，key为抖音id，value为达人在该平台id
            channel.queue_declare(queue='xingtu_kol_id', durable=True)  # 创建队列，发布达人的该平台id
            channel.basic_publish(exchange='',
                                  routing_key='xingtu_kol_id',
                                  body=id_xingtu,
                                  properties=pika.BasicProperties(
                                      delivery_mode=2,  # 消息持久化
                                  ))
        else:
            pass
        if list(db_kl.find({'core_user_id': id_douyin})) == []:    # 插入mongodb时去重.防止redis数据丢失，mongodb依然能够正常去重保存
            db_kl.insert(author)
        else:
            pass
</code></pre>
<p>开始消费：</p>
<pre><code class="language-python">if __name__ == '__main__':
    channel.basic_consume('xingtu_page',
                          callback,
                          auto_ack=False)
    channel.start_consuming()
</code></pre>
<h3 id="爬取达人详细数据get_kol_detailpy">爬取达人详细数据(get_kol_detail.py)</h3>
<p>消费者回调函数</p>
<pre><code class="language-python">def callback(ch, method, properties, body):
    try:
        '''调用get_kol_detail函数爬取'''
        get_kol_detail(body)
        time.sleep(30)
        ch.basic_ack(delivery_tag=method.delivery_tag)  # 消费者每次收到消息，要通知一声：已经收到，如果消费者连接断了，rabbitmq会重新把消息放到队列里，下次消费者可以连接的时候，就能重新收到丢失消息
    except:
        '''请求失败则把id重新放回队列'''
        channel.basic_publish(exchange='',
                              routing_key='xingtu_kol_id',
                              body=body,
                              properties=pika.BasicProperties(
                                  delivery_mode=2,  # 消息持久化
                              ))
        time.sleep(120)
</code></pre>
<p>根据达人在该平台ID爬取达人详细数据：</p>
<pre><code class="language-python">def get_kol_detail(id_xingtu):
    print id_xingtu
    headers = {'User-Agent': agent_proxy.get_agent()}
    cookies = open(u'/spiders/xingtu/cookie.txt','r').read()	# cookie写在cookie.txt里面
    cookies_dict = {}
    '''对原始cookies构造字典格式'''
    for line in cookies.split(';'):
        key, value = line.split('=', 1)  # 1代表只分一次，得到两个数据
        key = key.replace(' ', '')
        cookies_dict[key] = value
    '''列出所有待爬取url'''
    url_kol_index = '爬取的{}URL1'.format(id_xingtu) # 达人主页信息
    url_kol_order = '爬取的{}URL2'.format(id_xingtu) # 达人任务信息
    url_kol_item = '爬取的{}URL3'.format(id_xingtu)   # 达人个人作品数据，作品总数据，代表作品数据
    url_kol_video = '爬取的{}URL4'.format(id_xingtu)  # 达人近15条视频数据
    url_kol_daily_fans = '爬取的{}URL5'.format(id_xingtu)    # 达人粉丝趋势变化
    url_kol_fans_detail = '爬取的{}URL6'.format(id_xingtu)  # 达人粉丝详情
    url_dict = [{'kol_index': url_kol_index},
                {'kol_order': url_kol_order},
                {'kol_item': url_kol_item},
                {'kol_video': url_kol_video},
                {'kol_daily_fans': url_kol_daily_fans},
                {'kol_fans_detail': url_kol_fans_detail}]
    for url in url_dict:
        time.sleep(5)
        kol_detail = req.get( url.values()[0], headers=headers, cookies=cookies_dict, timeout=20, verify=False ).json()
        if kol_detail['msg'] == u'成功':
            db_kd.update( {'id': id_xingtu }, {'$set': { url.keys()[0] : kol_detail }}, upsert=True )  # upsert=True,则存在时修改，不存在时插入
        else:
            db_kd.update({'id': kol_detail['data']['nothing']}, {'$set': {url.keys()[0]: kol_detail}}, upsert=True) # 这行一定会报错，从而执行回调函数中的异常处理，因为kol_detail['data']['nothing']根本不存在
</code></pre>
<h3 id="实时更新达人详细数据send_id_apipy">实时更新达人详细数据(send_id_api.py)</h3>
<p>请求的json数据是该达人在抖音平台的ID（这里并未对请求的数据进行加密，如果是企业级开发的话对API的安全性和并发性能应做更加谨慎的设计），如果达人在该平台则提示「已添加」，否则提示「未收录」。</p>
<pre><code class="language-python">server = flask.Flask(__name__)
CORS(server, resources=r'/*')

@server.route('/douyinid', methods=['get', 'post'])
def registerPost():
    # post请求获取请求的参数，返回结果类型是json
    content = request.get_json()
    if list(db_kl.find({'core_user_id': int(content['id_douyin'].encode('utf-8'))})) == []: # content['id_douyin']为unicode编码，需要转成utf-8再int
        reply = jsonify({'Error': '该平台暂未收录该达人数据'})
    else:
        data_kol = pd.DataFrame(list(db_kl.find({'core_user_id': int(content['id_douyin'].encode('utf-8'))})))
        channel.basic_publish(exchange='',
                              routing_key='xingtu_kol_id',
                              body=data_kol.loc[0]['id'],  # 消息为该平台id
                              properties=pika.BasicProperties(
                                  delivery_mode=2,  # 消息持久化
                              ))
        reply = jsonify({'Success': '该达人已添加进爬取队列'})
    reply.headers['Access-Control-Allow-Origin'] = '*'
    reply.headers['Access-Control-Allow-Methods'] = 'POST'
    reply.headers['Access-Control-Allow-Headers'] = 'x-requested-with,content-type'
    return reply
</code></pre>
<p>启动flask API服务：</p>
<pre><code class="language-python">if __name__ == '__main__':
    WSGIServer(('0.0.0.0', 端口), server).serve_forever()
</code></pre>
<h2 id="启动项目">启动项目</h2>
<p>所有脚本的启动我就交给<strong>supervisor</strong>这个进程管理工具啦，配置也是相当地简单，每一个脚本的运行单独作为一个进程（当然可以通过定义类来把所有脚本写成一个框架，但进程相互独立也有好处，至少在没留log文件的时候调试起来要方便的多），在/spiders/xingtu/xingtu_supervisord.conf中写入以下配置：</p>
<pre><code class="language-config">[program:send_pages]
command=/spiders/env_py2.7/bin/python /spiders/xingtu/send_pages.py
#priority是权重，数字越大，优先级越高
priority=999
autostart=true
autorestart=unexpected
#startsecs表示进程启动多长时间后视为正常运行
startsecs=0
startretries=3
redirect_stderr=true
stdout_logfile=/tmp/stdout.log
stdout_logfile_maxbytes=10MB
stdout_logfile_backups=10

[program:get_kol_list]
command=/spiders/env_py2.7/bin/python /spiders/xingtu/get_kol_list.py
priority=998
autostart=true
autorestart=unexpected
startsecs=0
startretries=3
redirect_stderr=true
stdout_logfile=/tmp/stdout.log
stdout_logfile_maxbytes=10MB
stdout_logfile_backups=10

[program:get_kol_detail]
command=/spiders/env_py2.7/bin/python /spiders/xingtu/get_kol_detail.py
priority=997
autostart=true
autorestart=unexpected
startsecs=0
startretries=3
redirect_stderr=true
stdout_logfile=/tmp/stdout.log
stdout_logfile_maxbytes=10MB
stdout_logfile_backups=10

[program:send_id_api]
command=/spiders/env_py2.7/bin/python /spiders/xingtu/send_id_api.py
priority=996
autostart=true
autorestart=unexpected
startsecs=0
startretries=3
redirect_stderr=true
stdout_logfile=/tmp/stdout.log
stdout_logfile_maxbytes=10MB
stdout_logfile_backups=10
</code></pre>
<p>我的supervisor是在全局环境中的python2.7中安装的，激活环境：</p>
<pre><code class="language-shell">root@ubuntu:~# source activate python2.7
</code></pre>
<p>通过supervisor运行所有脚本：</p>
<pre><code class="language-shell">(python2.7) root@ubuntu:~# supervisord -c /spiders/xingtu/xingtu_supervisord.conf
</code></pre>
<p>查看supervisor的运行状态:</p>
<pre><code class="language-shell">(python2.7) root@ubuntu:~# supervisorctl -c /spiders/xingtu/xingtu_supervisord.conf status
get_kol_detail                   RUNNING   pid 100986, uptime 0:00:03
get_kol_list                     RUNNING   pid 100987, uptime 0:00:03
send_id_api                      RUNNING   pid 100985, uptime 0:00:03
send_pages                       EXITED    Jun 03 04:54 PM
</code></pre>
<p>我还可以通过浏览器访问supervisor的服务端，对所有进程进行重启，暂停等操作：
<img src="https://mohuazhan.github.io//post-images/1561024648823.png" alt="">
看一下RabbitMQ的控制面板：
<img src="https://mohuazhan.github.io//post-images/1561024615286.png" alt="">
查看一下redis：</p>
<pre><code class="language-shell">root@ubuntu:~# redis-cli -a 密码
127.0.0.1:6379&gt; hgetall hash_kolid 
  1) &quot;3891903335244664&quot;
  2) &quot;6680368576479625224&quot;
  3) &quot;59359411419&quot;
  4) &quot;6638835397155618820&quot;
  ......
</code></pre>
<p>找一个已经在数据库的达人ID，测试一下更新达人详细数据的API：
<img src="https://mohuazhan.github.io//post-images/1561024656872.png" alt="">
这个入门级的小爬虫，大功告成啦！</p>
<h2 id="写在后面">写在后面</h2>
<p>今天是高考的日子，前几天我无意中翻出了我去年毕业答辩的论文，想起了我的恩师们，同学们，我的大学舍友，历历在目。每一年毕业季会让人走向一座更高的山，跨向更广的海，但也让我们学会承受离别带来的伤感。</p>
<p>有时候我会后悔，为什么上学时候不好好学习，这个世界太多太多的魅力源自于<strong>science and technology</strong>，而自己只能羡慕别人的飞跃。但更多时候是欣喜，我的舍友既没有和我讨论薛定谔的猫也没有辩证忒休斯之船，但我记得，去年世界杯我们因为C罗的帽子戏法狂欢一宿，每年S系列赛的时候我们都会拿好小凳子围着给中国队加油，每次检查作业的前一晚都会手机闪光灯都会「派上用场」，每一次考试都会花光前面16周的记忆和运气......当流年成为历史，当老去成为定势，自己就会小心翼翼擦拭掉这些尘封回忆的灰，静默怀念。</p>
<p>OK，其实这部分后记本是前记，没有牢骚，就不会有这篇文章。我想说什么呢？正好可以用我毕业论文的一句话来表达：</p>
<blockquote>
<p>大学的路虽走到了终点，而求知求学的征途长路漫漫。</p>
</blockquote>
<p>记于2019/6/7，初夏</p>
]]></content>
    </entry>
</feed>